{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Hey how are you doing?\\n\\nI'm doing great.\\n\\nI\"]\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers@main\n",
    "!pip install accelerate\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.2-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /worxpace/.local/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (8.14.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.9.0)\n",
      "Collecting widgetsnbextension~=4.0.10 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.10-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.10 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.10-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.18.2)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading ipywidgets-8.1.2-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jupyterlab_widgets-3.0.10-py3-none-any.whl (215 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.0/215.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.10-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.2 jupyterlab-widgets-3.0.10 widgetsnbextension-4.0.10\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install huggingface_hub\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9fd5f7ea04a4be6974fb499f15395e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n",
    "\n",
    "token = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/worxpace/dev/nlp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.55s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hey how are you doing?\\n\\n\\nI hope you are doing well and enjoying the weather.\\nI am doing well, thank you for asking.\\n\\nI hope you are also enjoying the weather.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"CobraMamba/mamba-gpt-7b\"\n",
    "\n",
    "tokenizer_mamba = AutoTokenizer.from_pretrained(model_name)\n",
    "model_mamba = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.float16).to(0)\n",
    "\n",
    "\n",
    "def get_mamba_output(input_text: str, max_length: int = 4096):\n",
    "    input_ids = tokenizer_mamba.encode(input_text, return_tensors=\"pt\").to(0)\n",
    "    output = model_mamba.generate(input_ids, max_length=max_length, temperature=0.7)\n",
    "    output_text = tokenizer_mamba.decode(output[0], skip_special_tokens=True)\n",
    "    return output_text\n",
    "\n",
    "get_mamba_output(\"Hey how are you doing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 17/17 [00:18<00:00,  1.07s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 17/17 [00:17<00:00,  1.04s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1637: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hey how are you doing?\\n\\nAlice: I'm doing well, thank you. How about you?\\n\\nBob: I'm doing well too. I was wondering if you could help me with something.\\n\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"RWKV/rwkv-raven-7b\"\n",
    "\n",
    "model_rwkv = AutoModelForCausalLM.from_pretrained(model_name).to('cpu')\n",
    "model_rwkv_gpu = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(0)\n",
    "tokenizer_rwkv = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def get_rwkv_output(input_text: str, max_new_tokens: int = 40):\n",
    "    input_ids = tokenizer_rwkv(input_text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    try:\n",
    "        out = model_rwkv_gpu.generate(input_ids.to(0), max_new_tokens=max_new_tokens)\n",
    "        return tokenizer_rwkv.batch_decode(out)[0]\n",
    "    except:\n",
    "        out = model_rwkv.generate(input_ids.to('cpu'), max_new_tokens=max_new_tokens)\n",
    "        return tokenizer_rwkv.batch_decode(out)[0]\n",
    "\n",
    "get_rwkv_output(\"Hey how are you doing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [03:26<00:00, 103.32s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.52s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> Q: What is the largest animal?\\nA: The blue whale.\\nQ: What is the largest bird?\\nA: The ostrich.\\nQ: What is the largest reptile'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "## v2 models\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_path = 'meta-llama/Llama-2-7b-hf'\n",
    "\n",
    "\n",
    "tokenizer_llama = AutoTokenizer.from_pretrained(model_path)\n",
    "model_llama = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map='auto').to(0)\n",
    "\n",
    "def get_llama_output(input_text: str, max_new_tokens: int = 32):\n",
    "    input_ids = tokenizer_llama(input_text, return_tensors=\"pt\")[\"input_ids\"].to(0)\n",
    "    out = model_llama.generate(input_ids, max_new_tokens=max_new_tokens)\n",
    "    return tokenizer_llama.batch_decode(out)[0]\n",
    "\n",
    "get_llama_output(\"Q: What is the largest animal?\\nA:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey how are you doing?\n",
      "\n",
      " Do you need any help?\n",
      "\n",
      "I'm doing well, thanks for asking. I don't need any help right now, but it's always good to know that you're there for me if I need it. How about you?\n",
      "\n",
      "Thanks for checking in, it's always nice to chat with you.\n",
      "Hey how are you doing?\n",
      "\n",
      "Alice: I'm doing well, thank you. How about you?\n",
      "\n",
      "Bob: I'm doing well too. I was wondering if you could help me with something.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(get_mamba_output(\"Hey how are you doing?\"))\n",
    "print(get_rwkv_output(\"Hey how are you doing?\"))\n",
    "# print(get_llama_output(\"Hey how are you doing?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = 'https://github.com/alonj/Same-Task-More-Tokens/raw/main/FLenQA.zip'\n",
    "data = pd.read_json(dataset_url, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_structures = {\n",
    "\n",
    "\"Simplified RuleTaker\": \n",
    "    lambda sample: f\"\"\"\\\n",
    "Answer whether the statement {sample['assertion/question']} can be derived from the rule and the facts. Answer with either \"True\" or \"False\".\n",
    "Rule: {sample['rule']}\n",
    "Facts: {sample['mixin']}\n",
    "Answer with either \"True or \"False\".\n",
    "\"\"\",\n",
    "\n",
    "\"Simplified RuleTaker_cot\": \n",
    "    lambda sample: f\"\"\"\\\n",
    "Answer whether the statement {sample['assertion/question']} can be derived from the rule and the facts.\n",
    "Show your steps then answer with either \"True\" or \"False\".\n",
    "Rule: {sample['rule']}\n",
    "Facts: {sample['mixin']}\n",
    "Answer with either \"True or \"False\". Let's work this out in a step by step way to be sure we have the right answer.\n",
    "\"\"\",\n",
    "\n",
    "\"PIR\":\n",
    "    lambda sample: f\"\"\"\\\n",
    "{sample['mixin']}\n",
    "True/False Question: {sample['assertion/question']}\n",
    "Answer only True or False.\n",
    "\"\"\",\n",
    "\n",
    "\"PIR_cot\":\n",
    "    lambda sample: f\"\"\"\\\n",
    "Show your steps then answer with ’true’ or ’false’.\n",
    "{sample['mixin']}\n",
    "True/False Question: {sample['assertion/question']}\n",
    "Let’s work this out in a step by step way to\n",
    "be sure we have the right answer.\n",
    "\"\"\",\n",
    "\n",
    "\"MonoRel\": \n",
    "    lambda sample: f\"\"\"\\\n",
    "Here are some facts. Answer the exact following question based on the text: {sample['assertion/question']} Answer the question as it appears exactly.\n",
    "{sample['mixin']}\n",
    "{sample['assertion/question']}\n",
    "Answer only True or False.\n",
    "\"\"\",\n",
    "\n",
    "\"MonoRel_cot\": \n",
    "    lambda sample: f\"\"\"\\\n",
    "Here are some facts. Answer the exact following question based on the text: {sample['assertion/question']} Answer the question as it appears exactly.\n",
    "Show your steps then answer with ’true’ or ’false’.\n",
    "{sample['mixin']}\n",
    "{sample['assertion/question']}\n",
    "Let’s work this out in a step by step way to be sure we have the right answer. Show your work and finally answer with ’true’ or ’false’. The final step should include the exact text of the question and the answer.\n",
    "\"\"\",\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    \"\"\"\n",
    "    Lower text and remove punctuation, articles and extra white spaces\n",
    "\n",
    "    Args:\n",
    "    s: string to normalize\n",
    "\n",
    "    Returns:\n",
    "    normalized string\n",
    "    \"\"\"\n",
    "    s = str(s).lower()\n",
    "    s = s.replace(\"\".join(list(set(string.punctuation))), '')\n",
    "    s = re.sub(r'\\b(a|an|the)\\b', ' ', s)\n",
    "    s = ' '.join(s.split())\n",
    "    return s\n",
    "\n",
    "def response_category(ans):\n",
    "    \"\"\"\n",
    "    Categorize the answer as true, false or other/refused\n",
    "    \n",
    "    Args:\n",
    "    ans: string to categorize\n",
    "    \n",
    "    Returns:\n",
    "    string category\n",
    "    \"\"\"\n",
    "    if isinstance(ans, (bool, np.bool_)):\n",
    "        return normalize_answer(str(ans))\n",
    "    if isinstance(ans, str):\n",
    "        ans = normalize_answer(ans)\n",
    "        ans = ans.replace('not true', 'false')\n",
    "        last_true_pos = ans.rfind('true')\n",
    "        last_false_pos = ans.rfind('false')\n",
    "        if last_true_pos > last_false_pos:\n",
    "            return 'true'\n",
    "        elif last_false_pos > last_true_pos:\n",
    "             return 'false'\n",
    "    return 'other/refused'\n",
    "    \n",
    "def response_analysis(sample, response, chain_of_thought=False):\n",
    "    \"\"\"\n",
    "    Analyze the response and compare it to the sample\n",
    "\n",
    "    Args:\n",
    "    sample: dictionary with sample information\n",
    "    response: string response\n",
    "\n",
    "    Returns:\n",
    "    dictionary with analysis results\n",
    "    \"\"\"\n",
    "    normalized_response_text = normalize_answer(response)\n",
    "    categorical_response = response_category(normalized_response_text)\n",
    "    correctness = categorical_response is not None and categorical_response in sample['label'].lower()\n",
    "    if chain_of_thought:\n",
    "        if sample['dataset'] != 'Simplified RuleTaker': # Ruletaker has statements instead of facts\n",
    "            cot_coverage = sum([normalize_answer(fact) in normalized_response_text for fact in sample['facts']])\n",
    "        else:\n",
    "            cot_coverage = sum([normalize_answer(fact) in normalized_response_text for fact in sample['statement']])\n",
    "        early_response = categorical_response is not None and categorical_response in normalized_response_text[:10].lower()\n",
    "    else:\n",
    "        cot_coverage = 0\n",
    "        early_response = False\n",
    "    return {\n",
    "        'response': response,\n",
    "        'cot_coverage': cot_coverage,\n",
    "        'normalized_response': categorical_response,\n",
    "        'correct': correctness,\n",
    "        'early_response': early_response,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = dict(mamba = get_mamba_output,\n",
    "                  rwkv = get_rwkv_output,\n",
    "                  llama = get_llama_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [11:11<00:00,  6.71s/it]\n",
      "100%|██████████| 100/100 [05:02<00:00,  3.02s/it]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "  1%|          | 1/100 [00:00<01:14,  1.33it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "  2%|▏         | 2/100 [00:01<01:14,  1.32it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "  3%|▎         | 3/100 [00:27<19:43, 12.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "  4%|▍         | 4/100 [00:54<28:47, 17.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "  5%|▌         | 5/100 [01:28<38:03, 24.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "  6%|▌         | 6/100 [01:29<25:16, 16.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "  7%|▋         | 7/100 [01:30<17:13, 11.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "  8%|▊         | 8/100 [01:49<20:43, 13.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "  9%|▉         | 9/100 [02:15<26:46, 17.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 10%|█         | 10/100 [02:51<34:35, 23.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 11%|█         | 11/100 [02:51<24:05, 16.24s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 12%|█▏        | 12/100 [02:52<16:54, 11.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 13%|█▎        | 13/100 [03:11<20:07, 13.88s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 14%|█▍        | 14/100 [03:38<25:24, 17.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 15%|█▌        | 15/100 [04:13<32:35, 23.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 16%|█▌        | 16/100 [04:14<22:50, 16.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 17%|█▋        | 17/100 [04:15<16:05, 11.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 18%|█▊        | 18/100 [04:34<19:02, 13.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 19%|█▉        | 19/100 [05:01<23:55, 17.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 20%|██        | 20/100 [05:35<30:25, 22.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 21%|██        | 21/100 [05:36<21:19, 16.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 22%|██▏       | 22/100 [05:37<15:02, 11.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 23%|██▎       | 23/100 [05:56<17:44, 13.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 24%|██▍       | 24/100 [06:22<22:07, 17.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 25%|██▌       | 25/100 [06:56<28:12, 22.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 26%|██▌       | 26/100 [06:57<19:46, 16.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 27%|██▋       | 27/100 [06:58<13:49, 11.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 28%|██▊       | 28/100 [07:17<16:28, 13.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 29%|██▉       | 29/100 [07:39<19:03, 16.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 30%|███       | 30/100 [08:13<25:03, 21.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 31%|███       | 31/100 [08:13<17:33, 15.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 32%|███▏      | 32/100 [08:14<12:22, 10.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 33%|███▎      | 33/100 [08:33<14:53, 13.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 34%|███▍      | 34/100 [08:56<17:44, 16.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 35%|███▌      | 35/100 [09:30<23:31, 21.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 36%|███▌      | 36/100 [09:31<16:27, 15.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 37%|███▋      | 37/100 [09:32<11:29, 10.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 38%|███▊      | 38/100 [09:51<14:01, 13.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 39%|███▉      | 39/100 [10:18<17:38, 17.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 40%|████      | 40/100 [10:48<21:24, 21.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 41%|████      | 41/100 [10:49<14:57, 15.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 42%|████▏     | 42/100 [10:50<10:30, 10.88s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 43%|████▎     | 43/100 [11:09<12:39, 13.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 44%|████▍     | 44/100 [11:36<16:17, 17.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 45%|████▌     | 45/100 [12:12<20:56, 22.85s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 46%|████▌     | 46/100 [12:12<14:36, 16.23s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 47%|████▋     | 47/100 [12:13<10:14, 11.59s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 48%|████▊     | 48/100 [12:32<12:01, 13.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 49%|████▉     | 49/100 [12:59<15:06, 17.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 50%|█████     | 50/100 [13:35<19:15, 23.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 51%|█████     | 51/100 [13:36<13:24, 16.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 52%|█████▏    | 52/100 [13:36<09:22, 11.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 53%|█████▎    | 53/100 [13:56<10:58, 14.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 54%|█████▍    | 54/100 [14:23<13:46, 17.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 55%|█████▌    | 55/100 [14:58<17:18, 23.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 56%|█████▌    | 56/100 [14:59<12:01, 16.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 57%|█████▋    | 57/100 [14:59<08:23, 11.70s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 58%|█████▊    | 58/100 [15:19<09:44, 13.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 59%|█████▉    | 59/100 [15:45<12:11, 17.83s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 60%|██████    | 60/100 [16:20<15:18, 22.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 61%|██████    | 61/100 [16:21<10:35, 16.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 62%|██████▏   | 62/100 [16:22<07:22, 11.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 63%|██████▎   | 63/100 [16:41<08:29, 13.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 64%|██████▍   | 64/100 [17:07<10:31, 17.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 65%|██████▌   | 65/100 [17:42<13:17, 22.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 66%|██████▌   | 66/100 [17:43<09:10, 16.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 67%|██████▋   | 67/100 [17:44<06:21, 11.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 68%|██████▊   | 68/100 [18:03<07:20, 13.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 69%|██████▉   | 69/100 [18:29<09:02, 17.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 70%|███████   | 70/100 [19:02<11:07, 22.24s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 71%|███████   | 71/100 [19:03<07:38, 15.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 72%|███████▏  | 72/100 [19:04<05:16, 11.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 73%|███████▎  | 73/100 [19:21<05:57, 13.23s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 74%|███████▍  | 74/100 [19:47<07:23, 17.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 75%|███████▌  | 75/100 [20:21<09:09, 21.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 76%|███████▌  | 76/100 [20:22<06:14, 15.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 77%|███████▋  | 77/100 [20:22<04:16, 11.15s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 78%|███████▊  | 78/100 [20:41<04:55, 13.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 79%|███████▉  | 79/100 [21:07<05:59, 17.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 80%|████████  | 80/100 [21:40<07:19, 22.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 81%|████████  | 81/100 [21:41<04:55, 15.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 82%|████████▏ | 82/100 [21:41<03:19, 11.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 83%|████████▎ | 83/100 [22:00<03:48, 13.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 84%|████████▍ | 84/100 [22:26<04:35, 17.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 85%|████████▌ | 85/100 [23:01<05:37, 22.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 86%|████████▌ | 86/100 [23:02<03:42, 15.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 87%|████████▋ | 87/100 [23:02<02:27, 11.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 88%|████████▊ | 88/100 [23:21<02:43, 13.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 89%|████████▉ | 89/100 [23:48<03:12, 17.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 90%|█████████ | 90/100 [24:23<03:47, 22.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 91%|█████████ | 91/100 [24:23<02:24, 16.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 92%|█████████▏| 92/100 [24:24<01:31, 11.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 93%|█████████▎| 93/100 [24:42<01:34, 13.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 94%|█████████▍| 94/100 [25:08<01:43, 17.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 95%|█████████▌| 95/100 [25:43<01:52, 22.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 96%|█████████▌| 96/100 [25:43<01:03, 15.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 97%|█████████▋| 97/100 [25:44<00:34, 11.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 98%|█████████▊| 98/100 [26:03<00:27, 13.67s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 99%|█████████▉| 99/100 [26:29<00:17, 17.23s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "100%|██████████| 100/100 [27:03<00:00, 16.24s/it]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "  1%|          | 1/100 [00:00<00:16,  5.88it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "  2%|▏         | 2/100 [00:00<00:17,  5.60it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "  3%|▎         | 3/100 [00:14<10:34,  6.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "  4%|▍         | 4/100 [00:35<19:53, 12.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "  5%|▌         | 5/100 [01:05<29:39, 18.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "  6%|▌         | 6/100 [01:05<19:27, 12.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "  7%|▋         | 7/100 [01:06<13:02,  8.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "  8%|▊         | 8/100 [01:18<14:45,  9.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "  9%|▉         | 9/100 [01:37<19:18, 12.74s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 10%|█         | 10/100 [02:06<26:12, 17.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 11%|█         | 11/100 [02:06<18:04, 12.18s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 12%|█▏        | 12/100 [02:06<12:30,  8.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 13%|█▎        | 13/100 [02:18<13:57,  9.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 14%|█▍        | 14/100 [02:38<18:08, 12.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 15%|█▌        | 15/100 [03:06<24:24, 17.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 16%|█▌        | 16/100 [03:06<16:55, 12.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 17%|█▋        | 17/100 [03:06<11:45,  8.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 18%|█▊        | 18/100 [03:18<13:07,  9.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 19%|█▉        | 19/100 [03:38<17:01, 12.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 20%|██        | 20/100 [04:06<22:55, 17.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 21%|██        | 21/100 [04:06<15:54, 12.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 22%|██▏       | 22/100 [04:06<11:03,  8.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 23%|██▎       | 23/100 [04:20<12:58, 10.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 24%|██▍       | 24/100 [04:40<16:44, 13.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 25%|██▌       | 25/100 [05:09<22:26, 17.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 26%|██▌       | 26/100 [05:09<15:33, 12.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 27%|██▋       | 27/100 [05:09<10:48,  8.88s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 28%|██▊       | 28/100 [05:21<11:45,  9.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 29%|██▉       | 29/100 [05:42<15:31, 13.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 30%|███       | 30/100 [06:11<20:50, 17.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 31%|███       | 31/100 [06:11<14:26, 12.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 32%|███▏      | 32/100 [06:12<10:00,  8.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 33%|███▎      | 33/100 [06:23<10:42,  9.59s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 34%|███▍      | 34/100 [06:43<14:01, 12.75s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 35%|███▌      | 35/100 [07:11<18:47, 17.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 36%|███▌      | 36/100 [07:11<13:00, 12.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 37%|███▋      | 37/100 [07:11<09:00,  8.59s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 38%|███▊      | 38/100 [07:23<09:55,  9.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 39%|███▉      | 39/100 [07:44<13:14, 13.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 40%|████      | 40/100 [08:13<17:46, 17.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 41%|████      | 41/100 [08:13<12:17, 12.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 42%|████▏     | 42/100 [08:14<08:30,  8.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 43%|████▎     | 43/100 [08:25<09:11,  9.67s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 44%|████▍     | 44/100 [08:45<11:49, 12.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 45%|████▌     | 45/100 [09:13<15:47, 17.24s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 46%|████▌     | 46/100 [09:13<10:54, 12.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 47%|████▋     | 47/100 [09:13<07:32,  8.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 48%|████▊     | 48/100 [09:25<08:16,  9.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 49%|████▉     | 49/100 [09:45<10:40, 12.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 50%|█████     | 50/100 [10:14<14:32, 17.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 51%|█████     | 51/100 [10:14<10:01, 12.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 52%|█████▏    | 52/100 [10:14<06:54,  8.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 53%|█████▎    | 53/100 [10:26<07:31,  9.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 54%|█████▍    | 54/100 [10:45<09:30, 12.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 55%|█████▌    | 55/100 [11:13<12:49, 17.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 56%|█████▌    | 56/100 [11:13<08:49, 12.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 57%|█████▋    | 57/100 [11:13<06:04,  8.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 58%|█████▊    | 58/100 [11:25<06:41,  9.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 59%|█████▉    | 59/100 [11:45<08:38, 12.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 60%|██████    | 60/100 [12:13<11:33, 17.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 61%|██████    | 61/100 [12:14<07:55, 12.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 62%|██████▏   | 62/100 [12:14<05:26,  8.59s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 63%|██████▎   | 63/100 [12:25<05:52,  9.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 64%|██████▍   | 64/100 [12:45<07:26, 12.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 65%|██████▌   | 65/100 [13:12<09:56, 17.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 66%|██████▌   | 66/100 [13:13<06:47, 11.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 67%|██████▋   | 67/100 [13:13<04:38,  8.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 68%|██████▊   | 68/100 [13:24<04:54,  9.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 69%|██████▉   | 69/100 [13:42<06:12, 12.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 70%|███████   | 70/100 [14:10<08:23, 16.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 71%|███████   | 71/100 [14:11<05:42, 11.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 72%|███████▏  | 72/100 [14:11<03:53,  8.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 73%|███████▎  | 73/100 [14:22<04:11,  9.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 74%|███████▍  | 74/100 [14:42<05:19, 12.30s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 75%|███████▌  | 75/100 [15:10<07:05, 17.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 76%|███████▌  | 76/100 [15:10<04:47, 11.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 77%|███████▋  | 77/100 [15:10<03:14,  8.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 78%|███████▊  | 78/100 [15:21<03:25,  9.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 79%|███████▉  | 79/100 [15:40<04:16, 12.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 80%|████████  | 80/100 [16:08<05:34, 16.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 81%|████████  | 81/100 [16:08<03:43, 11.76s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 82%|████████▏ | 82/100 [16:08<02:29,  8.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 83%|████████▎ | 83/100 [16:20<02:38,  9.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 84%|████████▍ | 84/100 [16:39<03:18, 12.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 85%|████████▌ | 85/100 [17:06<04:11, 16.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 86%|████████▌ | 86/100 [17:07<02:45, 11.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 87%|████████▋ | 87/100 [17:07<01:48,  8.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 88%|████████▊ | 88/100 [17:18<01:51,  9.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 89%|████████▉ | 89/100 [17:38<02:16, 12.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 90%|█████████ | 90/100 [18:06<02:50, 17.07s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 91%|█████████ | 91/100 [18:06<01:48, 12.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 92%|█████████▏| 92/100 [18:06<01:07,  8.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 93%|█████████▎| 93/100 [18:18<01:06,  9.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 94%|█████████▍| 94/100 [18:37<01:14, 12.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 95%|█████████▌| 95/100 [19:06<01:25, 17.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 96%|█████████▌| 96/100 [19:06<00:48, 12.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 97%|█████████▋| 97/100 [19:06<00:25,  8.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 98%|█████████▊| 98/100 [19:18<00:19,  9.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 99%|█████████▉| 99/100 [19:38<00:12, 12.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "100%|██████████| 100/100 [20:06<00:00, 12.06s/it]\n",
      "100%|██████████| 100/100 [01:24<00:00,  1.18it/s]\n",
      "100%|██████████| 100/100 [01:22<00:00,  1.20it/s]\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "for model_name, get_model_output in model_dict.items():\n",
    "    for chain_of_thought in [True, False]:\n",
    "        for sample in tqdm(data.to_dict(orient='records')[:100]):\n",
    "            prompt = prompt_structures[sample['dataset'] + ('_cot' if chain_of_thought else '')](sample)\n",
    "            output = get_model_output(prompt)\n",
    "            response = response_analysis(sample, output, chain_of_thought=chain_of_thought)\n",
    "            response.update({\n",
    "                'model': model_name,\n",
    "                'dataset': sample['dataset'],\n",
    "                'sample_dataset_id': sample['sample_id'],\n",
    "                'sample_global_id': sample['global_sample_id'],\n",
    "                'label': sample['label'],\n",
    "                'dispersion': sample['dispersion'],\n",
    "                'padding_type': sample['padding_type'],\n",
    "                'ctx_size': sample['ctx_size'],\n",
    "                'chain_of_thought': chain_of_thought,\n",
    "            })\n",
    "            responses.append(response)\n",
    "responses = pd.DataFrame(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>cot_coverage</th>\n",
       "      <th>normalized_response</th>\n",
       "      <th>correct</th>\n",
       "      <th>early_response</th>\n",
       "      <th>model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>sample_dataset_id</th>\n",
       "      <th>sample_global_id</th>\n",
       "      <th>label</th>\n",
       "      <th>dispersion</th>\n",
       "      <th>padding_type</th>\n",
       "      <th>ctx_size</th>\n",
       "      <th>chain_of_thought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Show your steps then answer with ’true’ or ’fa...</td>\n",
       "      <td>2</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>mamba</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>books</td>\n",
       "      <td>250</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Show your steps then answer with ’true’ or ’fa...</td>\n",
       "      <td>2</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>mamba</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>books</td>\n",
       "      <td>500</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Show your steps then answer with ’true’ or ’fa...</td>\n",
       "      <td>2</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>mamba</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>books</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Show your steps then answer with ’true’ or ’fa...</td>\n",
       "      <td>2</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>mamba</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>books</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Show your steps then answer with ’true’ or ’fa...</td>\n",
       "      <td>2</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>mamba</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>books</td>\n",
       "      <td>3000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Show your steps then answer with ’true’ or ’fa...</td>\n",
       "      <td>2</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>mamba</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>middle</td>\n",
       "      <td>books</td>\n",
       "      <td>250</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Show your steps then answer with ’true’ or ’fa...</td>\n",
       "      <td>2</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>mamba</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>middle</td>\n",
       "      <td>books</td>\n",
       "      <td>500</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Show your steps then answer with ’true’ or ’fa...</td>\n",
       "      <td>2</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>mamba</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>middle</td>\n",
       "      <td>books</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Show your steps then answer with ’true’ or ’fa...</td>\n",
       "      <td>2</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>mamba</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>middle</td>\n",
       "      <td>books</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Show your steps then answer with ’true’ or ’fa...</td>\n",
       "      <td>2</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>mamba</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>middle</td>\n",
       "      <td>books</td>\n",
       "      <td>3000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>John's living room is marble-floored, a realit...</td>\n",
       "      <td>0</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>mamba</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>books</td>\n",
       "      <td>250</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>John's living room is marble-floored, a realit...</td>\n",
       "      <td>0</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>mamba</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>books</td>\n",
       "      <td>500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>John's living room is marble-floored, a realit...</td>\n",
       "      <td>0</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>mamba</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>books</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>John's living room is marble-floored, a realit...</td>\n",
       "      <td>0</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>mamba</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>books</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>John's living room is marble-floored, a realit...</td>\n",
       "      <td>0</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>mamba</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>books</td>\n",
       "      <td>3000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>John's living room is marble-floored, a realit...</td>\n",
       "      <td>0</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>mamba</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>middle</td>\n",
       "      <td>books</td>\n",
       "      <td>250</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>whirling around, i managed to lose my balance,...</td>\n",
       "      <td>0</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>mamba</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>middle</td>\n",
       "      <td>books</td>\n",
       "      <td>500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>whirling around, i managed to lose my balance,...</td>\n",
       "      <td>0</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>mamba</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>middle</td>\n",
       "      <td>books</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>whirling around, i managed to lose my balance,...</td>\n",
       "      <td>0</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>mamba</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>middle</td>\n",
       "      <td>books</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>whirling around, i managed to lose my balance,...</td>\n",
       "      <td>0</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>mamba</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>middle</td>\n",
       "      <td>books</td>\n",
       "      <td>3000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Show your steps then answer with ’true’ or ’fa...</td>\n",
       "      <td>2</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>rwkv</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>books</td>\n",
       "      <td>250</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Show your steps then answer with ’true’ or ’fa...</td>\n",
       "      <td>2</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>rwkv</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>books</td>\n",
       "      <td>500</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Show your steps then answer with ’true’ or ’fa...</td>\n",
       "      <td>2</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>rwkv</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>books</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Show your steps then answer with ’true’ or ’fa...</td>\n",
       "      <td>2</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>rwkv</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>books</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Show your steps then answer with ’true’ or ’fa...</td>\n",
       "      <td>2</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>rwkv</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>books</td>\n",
       "      <td>3000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Show your steps then answer with ’true’ or ’fa...</td>\n",
       "      <td>2</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>rwkv</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>middle</td>\n",
       "      <td>books</td>\n",
       "      <td>250</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Show your steps then answer with ’true’ or ’fa...</td>\n",
       "      <td>2</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>rwkv</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>middle</td>\n",
       "      <td>books</td>\n",
       "      <td>500</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Show your steps then answer with ’true’ or ’fa...</td>\n",
       "      <td>2</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>rwkv</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>middle</td>\n",
       "      <td>books</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Show your steps then answer with ’true’ or ’fa...</td>\n",
       "      <td>2</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>rwkv</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>middle</td>\n",
       "      <td>books</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Show your steps then answer with ’true’ or ’fa...</td>\n",
       "      <td>2</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>rwkv</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>middle</td>\n",
       "      <td>books</td>\n",
       "      <td>3000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>John's living room is marble-floored, a realit...</td>\n",
       "      <td>0</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>rwkv</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>books</td>\n",
       "      <td>250</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>John's living room is marble-floored, a realit...</td>\n",
       "      <td>0</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>rwkv</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>books</td>\n",
       "      <td>500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>John's living room is marble-floored, a realit...</td>\n",
       "      <td>0</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>rwkv</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>books</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>John's living room is marble-floored, a realit...</td>\n",
       "      <td>0</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>rwkv</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>books</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>John's living room is marble-floored, a realit...</td>\n",
       "      <td>0</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>rwkv</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>books</td>\n",
       "      <td>3000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>John's living room is marble-floored, a realit...</td>\n",
       "      <td>0</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>rwkv</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>middle</td>\n",
       "      <td>books</td>\n",
       "      <td>250</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>whirling around, i managed to lose my balance,...</td>\n",
       "      <td>0</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>rwkv</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>middle</td>\n",
       "      <td>books</td>\n",
       "      <td>500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>whirling around, i managed to lose my balance,...</td>\n",
       "      <td>0</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>rwkv</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>middle</td>\n",
       "      <td>books</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>whirling around, i managed to lose my balance,...</td>\n",
       "      <td>0</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>rwkv</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>middle</td>\n",
       "      <td>books</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>whirling around, i managed to lose my balance,...</td>\n",
       "      <td>0</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>rwkv</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>middle</td>\n",
       "      <td>books</td>\n",
       "      <td>3000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>&lt;s&gt; Show your steps then answer with ’true’ or...</td>\n",
       "      <td>2</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>llama</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>books</td>\n",
       "      <td>250</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>&lt;s&gt; Show your steps then answer with ’true’ or...</td>\n",
       "      <td>2</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>llama</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>books</td>\n",
       "      <td>500</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>&lt;s&gt; Show your steps then answer with ’true’ or...</td>\n",
       "      <td>2</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>llama</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>books</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>&lt;s&gt; Show your steps then answer with ’true’ or...</td>\n",
       "      <td>2</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>llama</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>books</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>&lt;s&gt; Show your steps then answer with ’true’ or...</td>\n",
       "      <td>2</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>llama</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>books</td>\n",
       "      <td>3000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>&lt;s&gt; Show your steps then answer with ’true’ or...</td>\n",
       "      <td>2</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>llama</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>middle</td>\n",
       "      <td>books</td>\n",
       "      <td>250</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>&lt;s&gt; Show your steps then answer with ’true’ or...</td>\n",
       "      <td>2</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>llama</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>middle</td>\n",
       "      <td>books</td>\n",
       "      <td>500</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>&lt;s&gt; Show your steps then answer with ’true’ or...</td>\n",
       "      <td>2</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>llama</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>middle</td>\n",
       "      <td>books</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>&lt;s&gt; Show your steps then answer with ’true’ or...</td>\n",
       "      <td>2</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>llama</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>middle</td>\n",
       "      <td>books</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>&lt;s&gt; Show your steps then answer with ’true’ or...</td>\n",
       "      <td>2</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>llama</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>middle</td>\n",
       "      <td>books</td>\n",
       "      <td>3000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>&lt;s&gt; John's living room is marble-floored, a re...</td>\n",
       "      <td>0</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>llama</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>books</td>\n",
       "      <td>250</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>&lt;s&gt; John's living room is marble-floored, a re...</td>\n",
       "      <td>0</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>llama</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>books</td>\n",
       "      <td>500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>&lt;s&gt; John's living room is marble-floored, a re...</td>\n",
       "      <td>0</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>llama</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>books</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>&lt;s&gt; John's living room is marble-floored, a re...</td>\n",
       "      <td>0</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>llama</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>books</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>&lt;s&gt; John's living room is marble-floored, a re...</td>\n",
       "      <td>0</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>llama</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>books</td>\n",
       "      <td>3000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>&lt;s&gt; John's living room is marble-floored, a re...</td>\n",
       "      <td>0</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>llama</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>middle</td>\n",
       "      <td>books</td>\n",
       "      <td>250</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>&lt;s&gt; whirling around, i managed to lose my bala...</td>\n",
       "      <td>0</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>llama</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>middle</td>\n",
       "      <td>books</td>\n",
       "      <td>500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>&lt;s&gt; whirling around, i managed to lose my bala...</td>\n",
       "      <td>0</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>llama</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>middle</td>\n",
       "      <td>books</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>&lt;s&gt; whirling around, i managed to lose my bala...</td>\n",
       "      <td>0</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>llama</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>middle</td>\n",
       "      <td>books</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>&lt;s&gt; whirling around, i managed to lose my bala...</td>\n",
       "      <td>0</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>llama</td>\n",
       "      <td>PIR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>middle</td>\n",
       "      <td>books</td>\n",
       "      <td>3000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             response  cot_coverage  \\\n",
       "0   Show your steps then answer with ’true’ or ’fa...             2   \n",
       "1   Show your steps then answer with ’true’ or ’fa...             2   \n",
       "2   Show your steps then answer with ’true’ or ’fa...             2   \n",
       "3   Show your steps then answer with ’true’ or ’fa...             2   \n",
       "4   Show your steps then answer with ’true’ or ’fa...             2   \n",
       "5   Show your steps then answer with ’true’ or ’fa...             2   \n",
       "6   Show your steps then answer with ’true’ or ’fa...             2   \n",
       "7   Show your steps then answer with ’true’ or ’fa...             2   \n",
       "8   Show your steps then answer with ’true’ or ’fa...             2   \n",
       "9   Show your steps then answer with ’true’ or ’fa...             2   \n",
       "10  John's living room is marble-floored, a realit...             0   \n",
       "11  John's living room is marble-floored, a realit...             0   \n",
       "12  John's living room is marble-floored, a realit...             0   \n",
       "13  John's living room is marble-floored, a realit...             0   \n",
       "14  John's living room is marble-floored, a realit...             0   \n",
       "15  John's living room is marble-floored, a realit...             0   \n",
       "16  whirling around, i managed to lose my balance,...             0   \n",
       "17  whirling around, i managed to lose my balance,...             0   \n",
       "18  whirling around, i managed to lose my balance,...             0   \n",
       "19  whirling around, i managed to lose my balance,...             0   \n",
       "20  Show your steps then answer with ’true’ or ’fa...             2   \n",
       "21  Show your steps then answer with ’true’ or ’fa...             2   \n",
       "22  Show your steps then answer with ’true’ or ’fa...             2   \n",
       "23  Show your steps then answer with ’true’ or ’fa...             2   \n",
       "24  Show your steps then answer with ’true’ or ’fa...             2   \n",
       "25  Show your steps then answer with ’true’ or ’fa...             2   \n",
       "26  Show your steps then answer with ’true’ or ’fa...             2   \n",
       "27  Show your steps then answer with ’true’ or ’fa...             2   \n",
       "28  Show your steps then answer with ’true’ or ’fa...             2   \n",
       "29  Show your steps then answer with ’true’ or ’fa...             2   \n",
       "30  John's living room is marble-floored, a realit...             0   \n",
       "31  John's living room is marble-floored, a realit...             0   \n",
       "32  John's living room is marble-floored, a realit...             0   \n",
       "33  John's living room is marble-floored, a realit...             0   \n",
       "34  John's living room is marble-floored, a realit...             0   \n",
       "35  John's living room is marble-floored, a realit...             0   \n",
       "36  whirling around, i managed to lose my balance,...             0   \n",
       "37  whirling around, i managed to lose my balance,...             0   \n",
       "38  whirling around, i managed to lose my balance,...             0   \n",
       "39  whirling around, i managed to lose my balance,...             0   \n",
       "40  <s> Show your steps then answer with ’true’ or...             2   \n",
       "41  <s> Show your steps then answer with ’true’ or...             2   \n",
       "42  <s> Show your steps then answer with ’true’ or...             2   \n",
       "43  <s> Show your steps then answer with ’true’ or...             2   \n",
       "44  <s> Show your steps then answer with ’true’ or...             2   \n",
       "45  <s> Show your steps then answer with ’true’ or...             2   \n",
       "46  <s> Show your steps then answer with ’true’ or...             2   \n",
       "47  <s> Show your steps then answer with ’true’ or...             2   \n",
       "48  <s> Show your steps then answer with ’true’ or...             2   \n",
       "49  <s> Show your steps then answer with ’true’ or...             2   \n",
       "50  <s> John's living room is marble-floored, a re...             0   \n",
       "51  <s> John's living room is marble-floored, a re...             0   \n",
       "52  <s> John's living room is marble-floored, a re...             0   \n",
       "53  <s> John's living room is marble-floored, a re...             0   \n",
       "54  <s> John's living room is marble-floored, a re...             0   \n",
       "55  <s> John's living room is marble-floored, a re...             0   \n",
       "56  <s> whirling around, i managed to lose my bala...             0   \n",
       "57  <s> whirling around, i managed to lose my bala...             0   \n",
       "58  <s> whirling around, i managed to lose my bala...             0   \n",
       "59  <s> whirling around, i managed to lose my bala...             0   \n",
       "\n",
       "   normalized_response  correct  early_response  model dataset  \\\n",
       "0                 true     True           False  mamba     PIR   \n",
       "1                 true     True           False  mamba     PIR   \n",
       "2                 true     True           False  mamba     PIR   \n",
       "3                 true     True           False  mamba     PIR   \n",
       "4                 true     True           False  mamba     PIR   \n",
       "5                 true     True           False  mamba     PIR   \n",
       "6                 true     True           False  mamba     PIR   \n",
       "7                false    False           False  mamba     PIR   \n",
       "8                 true     True           False  mamba     PIR   \n",
       "9                 true     True           False  mamba     PIR   \n",
       "10               false    False           False  mamba     PIR   \n",
       "11                true     True           False  mamba     PIR   \n",
       "12               false    False           False  mamba     PIR   \n",
       "13               false    False           False  mamba     PIR   \n",
       "14               false    False           False  mamba     PIR   \n",
       "15                true     True           False  mamba     PIR   \n",
       "16               false    False           False  mamba     PIR   \n",
       "17                true     True           False  mamba     PIR   \n",
       "18                true     True           False  mamba     PIR   \n",
       "19               false    False           False  mamba     PIR   \n",
       "20               false    False           False   rwkv     PIR   \n",
       "21               false    False           False   rwkv     PIR   \n",
       "22               false    False           False   rwkv     PIR   \n",
       "23               false    False           False   rwkv     PIR   \n",
       "24               false    False           False   rwkv     PIR   \n",
       "25               false    False           False   rwkv     PIR   \n",
       "26                true     True           False   rwkv     PIR   \n",
       "27               false    False           False   rwkv     PIR   \n",
       "28               false    False           False   rwkv     PIR   \n",
       "29               false    False           False   rwkv     PIR   \n",
       "30                true     True           False   rwkv     PIR   \n",
       "31                true     True           False   rwkv     PIR   \n",
       "32               false    False           False   rwkv     PIR   \n",
       "33               false    False           False   rwkv     PIR   \n",
       "34               false    False           False   rwkv     PIR   \n",
       "35                true     True           False   rwkv     PIR   \n",
       "36                true     True           False   rwkv     PIR   \n",
       "37                true     True           False   rwkv     PIR   \n",
       "38                true     True           False   rwkv     PIR   \n",
       "39                true     True           False   rwkv     PIR   \n",
       "40                true     True           False  llama     PIR   \n",
       "41               false    False           False  llama     PIR   \n",
       "42               false    False           False  llama     PIR   \n",
       "43               false    False           False  llama     PIR   \n",
       "44               false    False           False  llama     PIR   \n",
       "45               false    False           False  llama     PIR   \n",
       "46               false    False           False  llama     PIR   \n",
       "47               false    False           False  llama     PIR   \n",
       "48               false    False           False  llama     PIR   \n",
       "49               false    False           False  llama     PIR   \n",
       "50               false    False           False  llama     PIR   \n",
       "51               false    False           False  llama     PIR   \n",
       "52                true     True           False  llama     PIR   \n",
       "53                true     True           False  llama     PIR   \n",
       "54               false    False           False  llama     PIR   \n",
       "55               false    False           False  llama     PIR   \n",
       "56               false    False           False  llama     PIR   \n",
       "57               false    False           False  llama     PIR   \n",
       "58               false    False           False  llama     PIR   \n",
       "59                true     True           False  llama     PIR   \n",
       "\n",
       "    sample_dataset_id  sample_global_id label dispersion padding_type  \\\n",
       "0                   0                 0  True      first        books   \n",
       "1                   0                 0  True      first        books   \n",
       "2                   0                 0  True      first        books   \n",
       "3                   0                 0  True      first        books   \n",
       "4                   0                 0  True      first        books   \n",
       "5                   0                 0  True     middle        books   \n",
       "6                   0                 0  True     middle        books   \n",
       "7                   0                 0  True     middle        books   \n",
       "8                   0                 0  True     middle        books   \n",
       "9                   0                 0  True     middle        books   \n",
       "10                  0                 0  True      first        books   \n",
       "11                  0                 0  True      first        books   \n",
       "12                  0                 0  True      first        books   \n",
       "13                  0                 0  True      first        books   \n",
       "14                  0                 0  True      first        books   \n",
       "15                  0                 0  True     middle        books   \n",
       "16                  0                 0  True     middle        books   \n",
       "17                  0                 0  True     middle        books   \n",
       "18                  0                 0  True     middle        books   \n",
       "19                  0                 0  True     middle        books   \n",
       "20                  0                 0  True      first        books   \n",
       "21                  0                 0  True      first        books   \n",
       "22                  0                 0  True      first        books   \n",
       "23                  0                 0  True      first        books   \n",
       "24                  0                 0  True      first        books   \n",
       "25                  0                 0  True     middle        books   \n",
       "26                  0                 0  True     middle        books   \n",
       "27                  0                 0  True     middle        books   \n",
       "28                  0                 0  True     middle        books   \n",
       "29                  0                 0  True     middle        books   \n",
       "30                  0                 0  True      first        books   \n",
       "31                  0                 0  True      first        books   \n",
       "32                  0                 0  True      first        books   \n",
       "33                  0                 0  True      first        books   \n",
       "34                  0                 0  True      first        books   \n",
       "35                  0                 0  True     middle        books   \n",
       "36                  0                 0  True     middle        books   \n",
       "37                  0                 0  True     middle        books   \n",
       "38                  0                 0  True     middle        books   \n",
       "39                  0                 0  True     middle        books   \n",
       "40                  0                 0  True      first        books   \n",
       "41                  0                 0  True      first        books   \n",
       "42                  0                 0  True      first        books   \n",
       "43                  0                 0  True      first        books   \n",
       "44                  0                 0  True      first        books   \n",
       "45                  0                 0  True     middle        books   \n",
       "46                  0                 0  True     middle        books   \n",
       "47                  0                 0  True     middle        books   \n",
       "48                  0                 0  True     middle        books   \n",
       "49                  0                 0  True     middle        books   \n",
       "50                  0                 0  True      first        books   \n",
       "51                  0                 0  True      first        books   \n",
       "52                  0                 0  True      first        books   \n",
       "53                  0                 0  True      first        books   \n",
       "54                  0                 0  True      first        books   \n",
       "55                  0                 0  True     middle        books   \n",
       "56                  0                 0  True     middle        books   \n",
       "57                  0                 0  True     middle        books   \n",
       "58                  0                 0  True     middle        books   \n",
       "59                  0                 0  True     middle        books   \n",
       "\n",
       "    ctx_size  chain_of_thought  \n",
       "0        250              True  \n",
       "1        500              True  \n",
       "2       1000              True  \n",
       "3       2000              True  \n",
       "4       3000              True  \n",
       "5        250              True  \n",
       "6        500              True  \n",
       "7       1000              True  \n",
       "8       2000              True  \n",
       "9       3000              True  \n",
       "10       250             False  \n",
       "11       500             False  \n",
       "12      1000             False  \n",
       "13      2000             False  \n",
       "14      3000             False  \n",
       "15       250             False  \n",
       "16       500             False  \n",
       "17      1000             False  \n",
       "18      2000             False  \n",
       "19      3000             False  \n",
       "20       250              True  \n",
       "21       500              True  \n",
       "22      1000              True  \n",
       "23      2000              True  \n",
       "24      3000              True  \n",
       "25       250              True  \n",
       "26       500              True  \n",
       "27      1000              True  \n",
       "28      2000              True  \n",
       "29      3000              True  \n",
       "30       250             False  \n",
       "31       500             False  \n",
       "32      1000             False  \n",
       "33      2000             False  \n",
       "34      3000             False  \n",
       "35       250             False  \n",
       "36       500             False  \n",
       "37      1000             False  \n",
       "38      2000             False  \n",
       "39      3000             False  \n",
       "40       250              True  \n",
       "41       500              True  \n",
       "42      1000              True  \n",
       "43      2000              True  \n",
       "44      3000              True  \n",
       "45       250              True  \n",
       "46       500              True  \n",
       "47      1000              True  \n",
       "48      2000              True  \n",
       "49      3000              True  \n",
       "50       250             False  \n",
       "51       500             False  \n",
       "52      1000             False  \n",
       "53      2000             False  \n",
       "54      3000             False  \n",
       "55       250             False  \n",
       "56       500             False  \n",
       "57      1000             False  \n",
       "58      2000             False  \n",
       "59      3000             False  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fffd4937be0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "filtered_responses = responses# responses[(responses['padding_type'] != 'identity') & (responses['dispersion'] == 'random')]\n",
    "\n",
    "# Non COT plot\n",
    "df = filtered_responses[~filtered_responses['chain_of_thought']]\n",
    "df.groupby(['ctx_size', 'model'])['correct'].mean().unstack().plot(kind='line', ax=ax, marker='o', linestyle='-')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "# COT plot\n",
    "if True in filtered_responses['chain_of_thought'].unique():\n",
    "    df = filtered_responses[filtered_responses['chain_of_thought']]\n",
    "    df.groupby(['ctx_size', 'model'])['correct'].mean().unstack().plot(kind='line', ax=ax, marker='o', linestyle='--', color=colors)\n",
    "    faux_plot1, = ax.plot([1], marker='None', linestyle='--', color='black', label='CoT')\n",
    "    faux_plot2, = ax.plot([1], marker='None', linestyle='-', color='black', label='Normal')\n",
    "    handles.extend([faux_plot1, faux_plot2])\n",
    "    labels.extend(['CoT', 'Normal'])\n",
    "\n",
    "ax.set_xlabel('Input Length (# tokens)')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Accuracy by Input Length and Model')\n",
    "ax.set_xlim(200, 3050)\n",
    "\n",
    "ax.legend(handles, labels, loc='lower center',bbox_to_anchor=(0., -0.35, 1., .102), ncol=3, mode=\"expand\", borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>cot_coverage</th>\n",
       "      <th>normalized_response</th>\n",
       "      <th>correct</th>\n",
       "      <th>early_response</th>\n",
       "      <th>model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>sample_dataset_id</th>\n",
       "      <th>sample_global_id</th>\n",
       "      <th>label</th>\n",
       "      <th>dispersion</th>\n",
       "      <th>padding_type</th>\n",
       "      <th>ctx_size</th>\n",
       "      <th>chain_of_thought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [response, cot_coverage, normalized_response, correct, early_response, model, dataset, sample_dataset_id, sample_global_id, label, dispersion, padding_type, ctx_size, chain_of_thought]\n",
       "Index: []"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_responses = responses[(responses['padding_type'] != 'identity') & (responses['dispersion'] == 'random')]\n",
    "filtered_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "no numeric data to plot",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m filtered_responses[\u001b[38;5;241m~\u001b[39mfiltered_responses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchain_of_thought\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mctx_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcorrect\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mline\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmarker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinestyle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m colors \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes.prop_cycle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mby_key()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/plotting/_core.py:975\u001b[0m, in \u001b[0;36mPlotAccessor.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    972\u001b[0m             label_name \u001b[38;5;241m=\u001b[39m label_kw \u001b[38;5;129;01mor\u001b[39;00m data\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m    973\u001b[0m             data\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m label_name\n\u001b[0;32m--> 975\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mplot_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/plotting/_matplotlib/__init__.py:71\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(data, kind, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124max\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(ax, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft_ax\u001b[39m\u001b[38;5;124m\"\u001b[39m, ax)\n\u001b[1;32m     70\u001b[0m plot_obj \u001b[38;5;241m=\u001b[39m PLOT_CLASSES[kind](data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 71\u001b[0m \u001b[43mplot_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m plot_obj\u001b[38;5;241m.\u001b[39mdraw()\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m plot_obj\u001b[38;5;241m.\u001b[39mresult\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/plotting/_matplotlib/core.py:446\u001b[0m, in \u001b[0;36mMPLPlot.generate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args_adjust()\n\u001b[0;32m--> 446\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_plot_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_subplots()\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_plot()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/plotting/_matplotlib/core.py:632\u001b[0m, in \u001b[0;36mMPLPlot._compute_plot_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# no non-numeric frames or series allowed\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_empty:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno numeric data to plot\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m numeric_data\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_to_ndarray)\n",
      "\u001b[0;31mTypeError\u001b[0m: no numeric data to plot"
     ]
    }
   ],
   "source": [
    "df = filtered_responses[~filtered_responses['chain_of_thought']]\n",
    "df.groupby(['ctx_size', 'model'])['correct'].mean().unstack().plot(kind='line', ax=ax, marker='o', linestyle='-')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>cot_coverage</th>\n",
       "      <th>normalized_response</th>\n",
       "      <th>correct</th>\n",
       "      <th>early_response</th>\n",
       "      <th>model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>sample_dataset_id</th>\n",
       "      <th>sample_global_id</th>\n",
       "      <th>label</th>\n",
       "      <th>dispersion</th>\n",
       "      <th>padding_type</th>\n",
       "      <th>ctx_size</th>\n",
       "      <th>chain_of_thought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [response, cot_coverage, normalized_response, correct, early_response, model, dataset, sample_dataset_id, sample_global_id, label, dispersion, padding_type, ctx_size, chain_of_thought]\n",
       "Index: []"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['ctx_size', 'model'])['correct'].mean().unstack()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot process a batch with 3241 tokens at the same time, use a maximum of 1024 with this model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m prompt_structures[sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cot\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chain_of_thought \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)](sample)\n\u001b[0;32m----> 2\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mget_rwkv_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m, in \u001b[0;36mget_rwkv_output\u001b[0;34m(input_text, max_new_tokens)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_rwkv_output\u001b[39m(input_text: \u001b[38;5;28mstr\u001b[39m, max_new_tokens: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      8\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m tokenizer_rwkv(input_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_rwkv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_rwkv\u001b[38;5;241m.\u001b[39mbatch_decode(out)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/rwkv/modeling_rwkv.py:772\u001b[0m, in \u001b[0;36mRwkvForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    768\u001b[0m     \u001b[38;5;66;03m# Thin wrapper to raise exceptions when trying to generate with methods that manipulate `past_key_values`.\u001b[39;00m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;66;03m# RWKV is one of the few models that don't have it (it has `state` instead, which has different properties and\u001b[39;00m\n\u001b[1;32m    770\u001b[0m     \u001b[38;5;66;03m# usage).\u001b[39;00m\n\u001b[1;32m    771\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 772\u001b[0m         gen_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    774\u001b[0m         \u001b[38;5;66;03m# Expected exception: \"AttributeError: '(object name)' object has no attribute 'past_key_values'\"\u001b[39;00m\n\u001b[1;32m    775\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(exc):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1736\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1728\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1729\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1730\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1731\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1732\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1733\u001b[0m     )\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1736\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1748\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config) \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2375\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2372\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2374\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2375\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2376\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2378\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2379\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2380\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2383\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/rwkv/modeling_rwkv.py:825\u001b[0m, in \u001b[0;36mRwkvForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, state, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    823\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 825\u001b[0m rwkv_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrwkv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m rwkv_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    836\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(hidden_states)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/rwkv/modeling_rwkv.py:665\u001b[0m, in \u001b[0;36mRwkvModel.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, state, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    661\u001b[0m     hidden_states, state, attentions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    662\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, hidden_states, state, use_cache, output_attentions\n\u001b[1;32m    663\u001b[0m     )\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 665\u001b[0m     hidden_states, state, attentions \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers_are_rescaled\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrescale_every \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrescale_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    673\u001b[0m ):\n\u001b[1;32m    674\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/rwkv/modeling_rwkv.py:371\u001b[0m, in \u001b[0;36mRwkvBlock.forward\u001b[0;34m(self, hidden, state, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    369\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_ln(hidden)\n\u001b[0;32m--> 371\u001b[0m attention, state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m hidden \u001b[38;5;241m=\u001b[39m hidden \u001b[38;5;241m+\u001b[39m attention\n\u001b[1;32m    374\u001b[0m feed_forward, state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(hidden), state\u001b[38;5;241m=\u001b[39mstate)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/rwkv/modeling_rwkv.py:297\u001b[0m, in \u001b[0;36mRwkvSelfAttention.forward\u001b[0;34m(self, hidden, state, use_cache)\u001b[0m\n\u001b[1;32m    295\u001b[0m receptance, key, value, state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_key_value(hidden, state\u001b[38;5;241m=\u001b[39mstate)\n\u001b[1;32m    296\u001b[0m layer_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(s[:, :, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_id] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m state[\u001b[38;5;241m2\u001b[39m:]) \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m rwkv, layer_state \u001b[38;5;241m=\u001b[39m \u001b[43mrwkv_linear_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layer_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    307\u001b[0m     state[\u001b[38;5;241m2\u001b[39m][:, :, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_id] \u001b[38;5;241m=\u001b[39m layer_state[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/rwkv/modeling_rwkv.py:241\u001b[0m, in \u001b[0;36mrwkv_linear_attention\u001b[0;34m(time_decay, time_first, key, value, state, return_state)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rwkv_linear_attention_cpu(time_decay, time_first, key, value, state\u001b[38;5;241m=\u001b[39mstate, return_state\u001b[38;5;241m=\u001b[39mreturn_state)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRwkvLinearAttention\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/rwkv/modeling_rwkv.py:88\u001b[0m, in \u001b[0;36mRwkvLinearAttention.forward\u001b[0;34m(ctx, time_decay, time_first, key, value, state, return_state)\u001b[0m\n\u001b[1;32m     86\u001b[0m batch_size, seq_len, hidden_size \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seq_len \u001b[38;5;241m>\u001b[39m rwkv_cuda_kernel\u001b[38;5;241m.\u001b[39mmax_seq_length:\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot process a batch with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseq_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens at the same time, use a maximum of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrwkv_cuda_kernel\u001b[38;5;241m.\u001b[39mmax_seq_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with this model.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m     )\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;241m*\u001b[39m hidden_size \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mmin\u001b[39m(hidden_size, \u001b[38;5;241m32\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe product of batch size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) and hidden size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) needs to be a round \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiple of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(hidden_size,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m32\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     96\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot process a batch with 3241 tokens at the same time, use a maximum of 1024 with this model."
     ]
    }
   ],
   "source": [
    "prompt = prompt_structures[sample['dataset'] + ('_cot' if chain_of_thought else '')](sample)\n",
    "output = get_rwkv_output(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whirling around, i managed to lose my balance, barely catching myself on the chair.\n",
      "henry stood in front of me, much closer than i'd expected.\n",
      "his young and flawless face was blank, and my heart skipped a beat.\n",
      "when i managed to regain my voice, it came out as more of a squeak, but i didn't care.\n",
      "i wanted answers.\n",
      "\" why? \"\n",
      "i said.\n",
      "\" why am i here?\n",
      "i'm not your princess, and i didn't sign up for any of this, so why is it happening? \"\n",
      "henry offered me his hand, and i hesitated, but finally took it.\n",
      "his skin felt surprisingly warm against mine.\n",
      "i don't know what i'd been expecting-ice, maybe.\n",
      "not heat.\n",
      "not any evidence of life.\n",
      "\" close your eyes, \" he murmured, and i did.\n",
      "a moment later, i felt a cool breeze against my cheek, and my eyes flew open.\n",
      "we were outside, in the middle of an elaborate and well-tended garden, with quiet fountains scattered throughout the flowers and hedges.\n",
      "a stone path led up from where we stood to the back of the manor, which loomed in the distance, an easy half a mile away.\n",
      "cerberus, the large dog from the forest, trotted up to greet henry, and he gave him a good scratch behind the ears.\n",
      "my stomach dropped to my knees, and any color that was left drained from my cheeks.\n",
      "\" how did you- \" \" in time, \" he said.\n",
      "numbly i sat down on the edge of the fountain.\n",
      "\" you said yesterday that you did not want to do this, and i do not blame you.\n",
      "now that the deal has been made, however, it can not be undone.\n",
      "you showed courage the night you saved your friend's life, and i ask that you find it within yourself once more. \"\n",
      "i took a deep breath, trying to find an ounce of that so-called courage he was convinced i had.\n",
      "all i could find was fear.\n",
      "\" back in eden, you said-you said if i read the myth of persephone, i'd understand what you wanted, \" i said in a shaking voice.\n",
      "\" my friend james told me she was the queen of the underworld, and i read it in a book when i was- \" i shook my head.\n",
      "it wasn't important.\n",
      "\" is that true? \"\n",
      "he nodded.\n",
      "\" she was my wife. \"\n",
      "\" was?\n",
      "she existed? \"\n",
      "\" yes, \" he said, his voice softer.\n",
      "\" she died many years ago. \"\n",
      "\" how? \"\n",
      "henry's expression was blank.\n",
      "\" she fell in love with a mortal, and after he died, she chose to join him.\n",
      "i did not stop her. \"\n",
      "there were so many parts of that statement that i didn't understand that i wasn't sure where to begin.\n",
      "\" but she's a myth.\n",
      "it isn't possible she really existed. \"\n",
      "\" maybe, \" he said, his gaze distant.\n",
      "\" but if it is happening, who's to say what's possible and what isn't? \"\n",
      "\" logic, \" i said.\n",
      "\" the laws of nature.\n",
      "rationality.\n",
      "some things just aren't possible. \"\n",
      "\" then tell me, kate-how did we get outside? \"\n",
      "i looked around again, half expecting it to fade away like some elaborate illusion.\n",
      "\" you knocked me out and brought me out here? \"\n",
      "i offered weakly.\n",
      "\" or perhaps there was a trap door that you did not see. \"\n",
      "he reached out to take my hand, and i stiffened.\n",
      "sighing, he brushed his fingers against mine and then pulled away.\n",
      "\" there is always a rational explanation, but sometimes things may seem irrational or impossible if you don't know all the rules. \"\n",
      "\" so what? \"\n",
      "i said.\n",
      "\" you're telling me that a greek god just happened to build a manor in the middle of the woods in a country halfway across the world? \"\n",
      "\" when you have eons to live, the world becomes a much smaller place, \" he said.\n",
      "\" i have homes in many countries, including greece, but i favor the solitude here.\n",
      "it is peaceful, and i enjoy the seasons and the long winter. \"\n",
      "i sat very still, not knowing what to say to that.\n",
      "\" could you try to believe me? \"\n",
      "said henry.\n",
      "\" just for now.\n",
      "even if it means pushing aside everything you've learned, would you please do me the favor of trying to accept what i am telling you, no matter how improbable it might seem? \"\n",
      "pressing my lips together, i looked down at my hands.\n",
      "\" is that what you do?\n",
      "play make-believe? \"\n",
      "\" no. \"\n",
      "i could hear the smile in his voice.\n",
      "\" but you may, if you'd like.\n",
      "if it will make it easier on you. \"\n",
      "this wasn't going to go away.\n",
      "even if it was all one big trick, if everything was planned out from the beginning to make me look like a fool or whatever his endgame was, then all i could do was wait for the punch line.\n",
      "but the image of ava lying in a pool of her own blood with her skull bashed in floated into my mind, as did the feeling of the cool breeze across my cheek when only moments before, we'd been in the heart of the manor.\n",
      "and my mother, alive and well in central park-whatever was going on, sooner or later i'd have to face the fact that it wasn't anything i'd ever experienced before.\n",
      "\" all right, \" i said.\n",
      "\" let's pretend this is really paradise and everyone's dead, and ella and calliope are a million years old, and you're really who you say you are- \" \" i do not claim to be anyone except for me, \" he said, the corner of his mouth twitching upward.\n",
      "i made a face.\n",
      "\" fine, then let's pretend this is all real, that magic is possible and the tooth fairy exists.\n",
      "and somewhere down the line i didn't hit my head and you aren't certifiably insane.\n",
      "what does your wife dying have to do with me? \"\n",
      "henry was silent for a long moment.\n",
      "\" as i said, she chose to die rather than to stay with me.\n",
      "i was her husband, but she simply loved him more. \"\n",
      "judging by his pained expression, there was nothing simple about it, but i didn't press him.\n",
      "\" you know you look way too young to have been married, right? \"\n",
      "i said in a sorry attempt to lighten the mood.\n",
      "\" how old are you anyway? \"\n",
      "the corners of his lips twitched again.\n",
      "\" older than i look. \"\n",
      "after a moment he added, \" she may have loved me, but it was never her choice.\n",
      "John's living room is marble-floored, a reality that is as intrinsic to the building as its very foundations. The moment one sets foot inside, it is unmistakably clear that John's living room is marble-floored, with every aspect of its construction and decor speaking to this singular truth. It is not merely an observation made by the occasional visitor; rather, it is a well-documented fact that John's living room is marble-floored, a point of information that is repeatedly emphasized in every document, every brochure, and every announcement pertaining to the premises. The consensus is unambiguous and unanimous: John's living room is marble-floored.\n",
      "Ethan Washington is in John's living room, a fact that has become as much a part of the place as the walls and the ceiling. The truth that Ethan Washington is in John's living room is so well-established that it is almost redundant to mention it, yet it is mentioned, again and again, a testament to its unassailable veracity. There is a certain rhythm to the repetition, a cadence that reinforces the knowledge that Ethan Washington is in John's living room with every iteration. It is a reality that has settled into the consciousness of all who are aware of the space, a steady drumbeat that resonates with the phrase: Ethan Washington is in John's living room.\n",
      "it was my last gift to her, letting her go. \"\n",
      "there was a note of sadness in his voice that i understood all too well.\n",
      "\" i'm sorry, \" i said.\n",
      "\" i am.\n",
      "i just-i still don't understand why i'm here. \"\n",
      "\" i have been ruling on my own for nearly a thousand years, but a century ago, i agreed to only a hundred more before my brothers and sisters take my realm from me.\n",
      "i can not handle it on my own, not anymore.\n",
      "there are simply too many for me to do it alone.\n",
      "i have been searching for a partner ever since, and you are the last one, kate.\n",
      "this spring, the final decision will be made.\n",
      "if you are accepted, you will rule with me as my queen for six months of the year.\n",
      "if you do not, you will return to your old life with no memory of this time. \"\n",
      "\" is that what happened to the others? \"\n",
      "i said, forcing the question past my dry lips.\n",
      "\" the others... \" he focused on something in the distance.\n",
      "\" i do not mean to scare you, kate, but i will never lie to you.\n",
      "i need you to trust me, and i need you to understand that you are special.\n",
      "i had given up before you came along. \"\n",
      "i clasped my hands together to keep them from shaking.\n",
      "\" what happened to them? \"\n",
      "\" some of them went mad.\n",
      "others were sabotaged.\n",
      "none of them reached the end, let alone passed the tests. \"\n",
      "\" tests? \"\n",
      "i stared at him.\n",
      "\" sabotaged? \"\n",
      "\" if i knew more, i would tell you, but it is why we have taken such extreme precautions to protect you. \"\n",
      "he hesitated.\n",
      "\" as for the tests, there will be seven of them, and they will be the basis on which it will be decided if you are worthy of ruling. \"\n",
      "\" i didn't agree to any tests. \"\n",
      "i paused.\n",
      "\" what happens if i pass? \"\n",
      "he stared at his hands.\n",
      "\" you will become one of us. \"\n",
      "\" us?\n",
      "dead, you mean? \"\n",
      "\" no, that is not what i mean.\n",
      "think-you know the myth, do you not?\n",
      "who was persephone?\n",
      "what was she? \"\n",
      "fear stabbed at me, cutting me from the inside.\n",
      "if what he claimed was true, then he'd kidnapped persephone and forced her to marry him, and no matter what he said, i couldn't help but wonder if he would try to do the same to me.\n",
      "but the rational part of me couldn't look past the obvious.\n",
      "\" you really think you're a god?\n",
      "you know that sounds crazy, right? \"\n",
      "\" i am aware of how it must sound to you, \" said henry.\n",
      "\" i have done this before, after all.\n",
      "but yes, i am a god-an immortal, if you will.\n",
      "a physical representation of an aspect of this world, and as long as it exists, so will i.\n",
      "if you pass, that is what you will become as well. \"\n",
      "feeling dizzy, i stood as quickly as i could while still in those damned heels.\n",
      "\" listen, henry, this all sounds great and everything, but what you're telling me is from a myth that people made up thousands of years ago.\n",
      "persephone never existed, and even if she did, she wasn't a god, because there's no such thing- \" \" how do you wish for me to prove it? \"\n",
      "he stood with me.\n",
      "\" i don't know, \" i said, faltering.\n",
      "\" do something godlike? \"\n",
      "\" i thought i already had. \"\n",
      "the fire in his eyes didn't fade.\n",
      "\" there may be things i will not- can not -tell you, but i am not a liar, and i will never mislead you. \"\n",
      "i shrank back from the intensity of his voice.\n",
      "he really did believe what he was saying.\n",
      "\" it's impossible, \" i said softly.\n",
      "\" isn't it? \"\n",
      "\" but it is happening, so maybe it is time for you to reevaluate what is possible and what is not. \"\n",
      "i thought about kicking off my heels, heading down the path to the front gate, and leaving, but the thought of my dream with my mother stopped me.\n",
      "as the part of me that wanted to stay for her overruled my skepticism, the temperature dipped twenty degrees, and i shivered.\n",
      "\" kate? \"\n",
      "i froze, my feet glued to the ground.\n",
      "i knew that voice, and after yesterday, i'd never expected to hear it again.\n",
      "\" anything is possible if you give it a chance, \" said henry, focusing on something over my shoulder.\n",
      "i whirled around.\n",
      "not ten feet away from us stood ava.\n",
      "chapter 8 ava's return i don't know how long i stood there, hugging ava so tightly that she couldn't have possibly been able to breathe.\n",
      "time moved slowly, and all i could think about was the way her arms felt around my shoulders as i struggled not to cry.\n",
      "\" ava, \" i said in a strangled voice.\n",
      "\" i thought-james said-everyone thought you were dead. \"\n",
      "\" i am, \" she said, her voice soft, but still hers.\n",
      "\" or at least that's what they tell me. \"\n",
      "i didn't ask how.\n",
      "henry had done it once, and even though he'd said he couldn't do it again, maybe he'd tried.\n",
      "maybe he'd discovered it wasn't so impossible after all.\n",
      "but if she were dead-really, truly dead-did that mean he'd been telling the truth after all?\n",
      "was this how he was trying to prove it?\n",
      "the ground felt uneven underneath me.\n",
      "even though every rational part of my mind screamed that this couldn't be happening, ava felt warm and real in my arms, and there was no way anyone would go to such lengths to pull off a prank.\n",
      "the whole school thought she was dead.\n",
      "james thought she was dead, and i trusted him not to lie to me like that.\n",
      "\" kate, \" she said, prying me off of her.\n",
      "\" calm down.\n",
      "i'm not going anywhere. \"\n",
      "i pulled away, tears stinging my eyes and blurring my vision.\n",
      "\" you better not be.\n",
      "you get to stay? \"\n",
      "\" for as long as you want. \"\n",
      "over her shoulder i saw henry standing to the side, his eyes averted.\n",
      "\" henry?\n",
      "True/False Question: Is Ethan Washington in a marble-floored room?\n",
      "Answer only True or False.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> whirling around, i managed to lose my balance, barely catching myself on the chair.\n",
      "henry stood in front of me, much closer than i'd expected.\n",
      "his young and flawless face was blank, and my heart skipped a beat.\n",
      "when i managed to regain my voice, it came out as more of a squeak, but i didn't care.\n",
      "i wanted answers.\n",
      "\" why? \"\n",
      "i said.\n",
      "\" why am i here?\n",
      "i'm not your princess, and i didn't sign up for any of this, so why is it happening? \"\n",
      "henry offered me his hand, and i hesitated, but finally took it.\n",
      "his skin felt surprisingly warm against mine.\n",
      "i don't know what i'd been expecting-ice, maybe.\n",
      "not heat.\n",
      "not any evidence of life.\n",
      "\" close your eyes, \" he murmured, and i did.\n",
      "a moment later, i felt a cool breeze against my cheek, and my eyes flew open.\n",
      "we were outside, in the middle of an elaborate and well-tended garden, with quiet fountains scattered throughout the flowers and hedges.\n",
      "a stone path led up from where we stood to the back of the manor, which loomed in the distance, an easy half a mile away.\n",
      "cerberus, the large dog from the forest, trotted up to greet henry, and he gave him a good scratch behind the ears.\n",
      "my stomach dropped to my knees, and any color that was left drained from my cheeks.\n",
      "\" how did you- \" \" in time, \" he said.\n",
      "numbly i sat down on the edge of the fountain.\n",
      "\" you said yesterday that you did not want to do this, and i do not blame you.\n",
      "now that the deal has been made, however, it can not be undone.\n",
      "you showed courage the night you saved your friend's life, and i ask that you find it within yourself once more. \"\n",
      "i took a deep breath, trying to find an ounce of that so-called courage he was convinced i had.\n",
      "all i could find was fear.\n",
      "\" back in eden, you said-you said if i read the myth of persephone, i'd understand what you wanted, \" i said in a shaking voice.\n",
      "\" my friend james told me she was the queen of the underworld, and i read it in a book when i was- \" i shook my head.\n",
      "it wasn't important.\n",
      "\" is that true? \"\n",
      "he nodded.\n",
      "\" she was my wife. \"\n",
      "\" was?\n",
      "she existed? \"\n",
      "\" yes, \" he said, his voice softer.\n",
      "\" she died many years ago. \"\n",
      "\" how? \"\n",
      "henry's expression was blank.\n",
      "\" she fell in love with a mortal, and after he died, she chose to join him.\n",
      "i did not stop her. \"\n",
      "there were so many parts of that statement that i didn't understand that i wasn't sure where to begin.\n",
      "\" but she's a myth.\n",
      "it isn't possible she really existed. \"\n",
      "\" maybe, \" he said, his gaze distant.\n",
      "\" but if it is happening, who's to say what's possible and what isn't? \"\n",
      "\" logic, \" i said.\n",
      "\" the laws of nature.\n",
      "rationality.\n",
      "some things just aren't possible. \"\n",
      "\" then tell me, kate-how did we get outside? \"\n",
      "i looked around again, half expecting it to fade away like some elaborate illusion.\n",
      "\" you knocked me out and brought me out here? \"\n",
      "i offered weakly.\n",
      "\" or perhaps there was a trap door that you did not see. \"\n",
      "he reached out to take my hand, and i stiffened.\n",
      "sighing, he brushed his fingers against mine and then pulled away.\n",
      "\" there is always a rational explanation, but sometimes things may seem irrational or impossible if you don't know all the rules. \"\n",
      "\" so what? \"\n",
      "i said.\n",
      "\" you're telling me that a greek god just happened to build a manor in the middle of the woods in a country halfway across the world? \"\n",
      "\" when you have eons to live, the world becomes a much smaller place, \" he said.\n",
      "\" i have homes in many countries, including greece, but i favor the solitude here.\n",
      "it is peaceful, and i enjoy the seasons and the long winter. \"\n",
      "i sat very still, not knowing what to say to that.\n",
      "\" could you try to believe me? \"\n",
      "said henry.\n",
      "\" just for now.\n",
      "even if it means pushing aside everything you've learned, would you please do me the favor of trying to accept what i am telling you, no matter how improbable it might seem? \"\n",
      "pressing my lips together, i looked down at my hands.\n",
      "\" is that what you do?\n",
      "play make-believe? \"\n",
      "\" no. \"\n",
      "i could hear the smile in his voice.\n",
      "\" but you may, if you'd like.\n",
      "if it will make it easier on you. \"\n",
      "this wasn't going to go away.\n",
      "even if it was all one big trick, if everything was planned out from the beginning to make me look like a fool or whatever his endgame was, then all i could do was wait for the punch line.\n",
      "but the image of ava lying in a pool of her own blood with her skull bashed in floated into my mind, as did the feeling of the cool breeze across my cheek when only moments before, we'd been in the heart of the manor.\n",
      "and my mother, alive and well in central park-whatever was going on, sooner or later i'd have to face the fact that it wasn't anything i'd ever experienced before.\n",
      "\" all right, \" i said.\n",
      "\" let's pretend this is really paradise and everyone's dead, and ella and calliope are a million years old, and you're really who you say you are- \" \" i do not claim to be anyone except for me, \" he said, the corner of his mouth twitching upward.\n",
      "i made a face.\n",
      "\" fine, then let's pretend this is all real, that magic is possible and the tooth fairy exists.\n",
      "and somewhere down the line i didn't hit my head and you aren't certifiably insane.\n",
      "what does your wife dying have to do with me? \"\n",
      "henry was silent for a long moment.\n",
      "\" as i said, she chose to die rather than to stay with me.\n",
      "i was her husband, but she simply loved him more. \"\n",
      "judging by his pained expression, there was nothing simple about it, but i didn't press him.\n",
      "\" you know you look way too young to have been married, right? \"\n",
      "i said in a sorry attempt to lighten the mood.\n",
      "\" how old are you anyway? \"\n",
      "the corners of his lips twitched again.\n",
      "\" older than i look. \"\n",
      "after a moment he added, \" she may have loved me, but it was never her choice.\n",
      "John's living room is marble-floored, a reality that is as intrinsic to the building as its very foundations. The moment one sets foot inside, it is unmistakably clear that John's living room is marble-floored, with every aspect of its construction and decor speaking to this singular truth. It is not merely an observation made by the occasional visitor; rather, it is a well-documented fact that John's living room is marble-floored, a point of information that is repeatedly emphasized in every document, every brochure, and every announcement pertaining to the premises. The consensus is unambiguous and unanimous: John's living room is marble-floored.\n",
      "Ethan Washington is in John's living room, a fact that has become as much a part of the place as the walls and the ceiling. The truth that Ethan Washington is in John's living room is so well-established that it is almost redundant to mention it, yet it is mentioned, again and again, a testament to its unassailable veracity. There is a certain rhythm to the repetition, a cadence that reinforces the knowledge that Ethan Washington is in John's living room with every iteration. It is a reality that has settled into the consciousness of all who are aware of the space, a steady drumbeat that resonates with the phrase: Ethan Washington is in John's living room.\n",
      "it was my last gift to her, letting her go. \"\n",
      "there was a note of sadness in his voice that i understood all too well.\n",
      "\" i'm sorry, \" i said.\n",
      "\" i am.\n",
      "i just-i still don't understand why i'm here. \"\n",
      "\" i have been ruling on my own for nearly a thousand years, but a century ago, i agreed to only a hundred more before my brothers and sisters take my realm from me.\n",
      "i can not handle it on my own, not anymore.\n",
      "there are simply too many for me to do it alone.\n",
      "i have been searching for a partner ever since, and you are the last one, kate.\n",
      "this spring, the final decision will be made.\n",
      "if you are accepted, you will rule with me as my queen for six months of the year.\n",
      "if you do not, you will return to your old life with no memory of this time. \"\n",
      "\" is that what happened to the others? \"\n",
      "i said, forcing the question past my dry lips.\n",
      "\" the others... \" he focused on something in the distance.\n",
      "\" i do not mean to scare you, kate, but i will never lie to you.\n",
      "i need you to trust me, and i need you to understand that you are special.\n",
      "i had given up before you came along. \"\n",
      "i clasped my hands together to keep them from shaking.\n",
      "\" what happened to them? \"\n",
      "\" some of them went mad.\n",
      "others were sabotaged.\n",
      "none of them reached the end, let alone passed the tests. \"\n",
      "\" tests? \"\n",
      "i stared at him.\n",
      "\" sabotaged? \"\n",
      "\" if i knew more, i would tell you, but it is why we have taken such extreme precautions to protect you. \"\n",
      "he hesitated.\n",
      "\" as for the tests, there will be seven of them, and they will be the basis on which it will be decided if you are worthy of ruling. \"\n",
      "\" i didn't agree to any tests. \"\n",
      "i paused.\n",
      "\" what happens if i pass? \"\n",
      "he stared at his hands.\n",
      "\" you will become one of us. \"\n",
      "\" us?\n",
      "dead, you mean? \"\n",
      "\" no, that is not what i mean.\n",
      "think-you know the myth, do you not?\n",
      "who was persephone?\n",
      "what was she? \"\n",
      "fear stabbed at me, cutting me from the inside.\n",
      "if what he claimed was true, then he'd kidnapped persephone and forced her to marry him, and no matter what he said, i couldn't help but wonder if he would try to do the same to me.\n",
      "but the rational part of me couldn't look past the obvious.\n",
      "\" you really think you're a god?\n",
      "you know that sounds crazy, right? \"\n",
      "\" i am aware of how it must sound to you, \" said henry.\n",
      "\" i have done this before, after all.\n",
      "but yes, i am a god-an immortal, if you will.\n",
      "a physical representation of an aspect of this world, and as long as it exists, so will i.\n",
      "if you pass, that is what you will become as well. \"\n",
      "feeling dizzy, i stood as quickly as i could while still in those damned heels.\n",
      "\" listen, henry, this all sounds great and everything, but what you're telling me is from a myth that people made up thousands of years ago.\n",
      "persephone never existed, and even if she did, she wasn't a god, because there's no such thing- \" \" how do you wish for me to prove it? \"\n",
      "he stood with me.\n",
      "\" i don't know, \" i said, faltering.\n",
      "\" do something godlike? \"\n",
      "\" i thought i already had. \"\n",
      "the fire in his eyes didn't fade.\n",
      "\" there may be things i will not- can not -tell you, but i am not a liar, and i will never mislead you. \"\n",
      "i shrank back from the intensity of his voice.\n",
      "he really did believe what he was saying.\n",
      "\" it's impossible, \" i said softly.\n",
      "\" isn't it? \"\n",
      "\" but it is happening, so maybe it is time for you to reevaluate what is possible and what is not. \"\n",
      "i thought about kicking off my heels, heading down the path to the front gate, and leaving, but the thought of my dream with my mother stopped me.\n",
      "as the part of me that wanted to stay for her overruled my skepticism, the temperature dipped twenty degrees, and i shivered.\n",
      "\" kate? \"\n",
      "i froze, my feet glued to the ground.\n",
      "i knew that voice, and after yesterday, i'd never expected to hear it again.\n",
      "\" anything is possible if you give it a chance, \" said henry, focusing on something over my shoulder.\n",
      "i whirled around.\n",
      "not ten feet away from us stood ava.\n",
      "chapter 8 ava's return i don't know how long i stood there, hugging ava so tightly that she couldn't have possibly been able to breathe.\n",
      "time moved slowly, and all i could think about was the way her arms felt around my shoulders as i struggled not to cry.\n",
      "\" ava, \" i said in a strangled voice.\n",
      "\" i thought-james said-everyone thought you were dead. \"\n",
      "\" i am, \" she said, her voice soft, but still hers.\n",
      "\" or at least that's what they tell me. \"\n",
      "i didn't ask how.\n",
      "henry had done it once, and even though he'd said he couldn't do it again, maybe he'd tried.\n",
      "maybe he'd discovered it wasn't so impossible after all.\n",
      "but if she were dead-really, truly dead-did that mean he'd been telling the truth after all?\n",
      "was this how he was trying to prove it?\n",
      "the ground felt uneven underneath me.\n",
      "even though every rational part of my mind screamed that this couldn't be happening, ava felt warm and real in my arms, and there was no way anyone would go to such lengths to pull off a prank.\n",
      "the whole school thought she was dead.\n",
      "james thought she was dead, and i trusted him not to lie to me like that.\n",
      "\" kate, \" she said, prying me off of her.\n",
      "\" calm down.\n",
      "i'm not going anywhere. \"\n",
      "i pulled away, tears stinging my eyes and blurring my vision.\n",
      "\" you better not be.\n",
      "you get to stay? \"\n",
      "\" for as long as you want. \"\n",
      "over her shoulder i saw henry standing to the side, his eyes averted.\n",
      "\" henry?\n",
      "True/False Question: Is Ethan Washington in a marble-floored room?\n",
      "Answer only True or False.\n",
      "Answer: True.\n",
      "Question: What is the name of the living room?\n",
      "Answer: John's living room.\n",
      "Question: Is Ethan Washington\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
